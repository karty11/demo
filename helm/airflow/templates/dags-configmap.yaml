apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-dags
  namespace: airflow
  labels:
    app: airflow
data:
  mysql_to_minio_parquet.py: |
    from airflow import DAG
    from airflow.operators.python import PythonOperator
    from airflow.providers.mysql.hooks.mysql import MySqlHook
    from airflow.providers.amazon.aws.hooks.s3 import S3Hook
    from airflow.providers.trino.operators.trino import TrinoOperator
    from datetime import datetime
    import pandas as pd
    import os, requests

    def extract_mysql(**context):
        hook = MySqlHook(mysql_conn_id="mysql_conn")
        df = hook.get_pandas_df("SELECT id, user_id, amount, created_at FROM transactions")
        local_path = "/tmp/transactions.csv"
        df.to_csv(local_path, index=False)
        context['ti'].xcom_push(key="local_csv", value=local_path)

    def upload_to_minio(**context):
        s3_hook = S3Hook(aws_conn_id="minio_conn")
        local_csv = context['ti'].xcom_pull(task_ids="extract", key="local_csv")
        s3_hook.load_file(
            filename=local_csv,
            bucket_name="datalake",
            key=f"raw/transactions_{datetime.now().strftime('%Y%m%d%H%M%S')}.csv",
            replace=True
        )

    def transform_to_parquet(**context):
        local_csv = context['ti'].xcom_pull(task_ids="extract", key="local_csv")
        df = pd.read_csv(local_csv)
        df['day'] = pd.to_datetime(df['created_at']).dt.date
        parquet_file = "/tmp/transactions.parquet"
        df.to_parquet(parquet_file, index=False)
        s3_hook = S3Hook(aws_conn_id="minio_conn")
        s3_hook.load_file(
            filename=parquet_file,
            bucket_name="datalake",
            key=f"curated/transactions_{datetime.now().strftime('%Y%m%d%H%M%S')}.parquet",
            replace=True
        )

    def notify_grafana(**context):
        url = os.environ.get("GRAFANA_API_URL") + "/api/annotations"
        headers = {
            "Authorization": f"Bearer {os.environ.get('GRAFANA_API_KEY')}",
            "Content-Type": "application/json"
        }
        payload = {
            "tags": ["airflow", "datalake"],
            "text": f"DAG {context['dag'].dag_id} finished at {context['ts']}"
        }
        resp = requests.post(url, headers=headers, json=payload)
        if resp.status_code not in (200, 201):
            raise Exception(f"Grafana notify failed: {resp.status_code}, {resp.text}")

    default_args = {
        "owner": "airflow",
        "start_date": datetime(2025, 1, 1),
        "retries": 1,
    }

    with DAG(
        dag_id="mysql_to_minio_parquet",
        default_args=default_args,
        schedule_interval="@daily",
        catchup=False,
    ) as dag:

        extract = PythonOperator(
            task_id="extract",
            python_callable=extract_mysql
        )

        upload = PythonOperator(
            task_id="upload_to_minio",
            python_callable=upload_to_minio
        )

        transform = PythonOperator(
            task_id="transform_parquet",
            python_callable=transform_to_parquet
        )

        notify = PythonOperator(
            task_id="notify_grafana",
            python_callable=notify_grafana
        )

        extract >> upload >> transform >> notify

